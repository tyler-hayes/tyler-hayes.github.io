
<!DOCTYPE html>
<html lang="en">

<head>
    <title>Tyler L. Hayes</title>
    <description>Tyler L. Hayes Personal Website</description>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="tyler, hayes, rit, rochester institute of technology, new york, computer vision, deep-learning, machine-learning, machine learning, deep learning, artificial intelligence, AI, segmentation, image, graph, manifold, math, RIT, Rochester, rochester, lifelong learning, continual learning">
    <meta name="author" content="Tyler Hayes">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-151327506-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-151327506-1');
    </script>

    <script>
        /**
         * Function that registers a click on an outbound link in Analytics.
         * This function takes a valid URL string as an argument, and uses that URL string
         * as the event label. Setting the transport method to 'beacon' lets the hit be sent
         * using 'navigator.sendBeacon' in browser that support it.
         */
        var getOutboundLink = function(url) {
			if (window.gtag && gtag.loaded)
			{
				gtag('event', 'click', {
					'event_category': 'outbound',
					'event_label': url,
					'transport_type': 'beacon',
					'event_callback': function() {
						document.location = url;
					}
				});
			}
			else
			{
				document.location = url;
			}
        }
    </script>

    <style>
        .newflag {
            background-color: #C0B283;
            color: #ffffff;
            padding: 2px 5px;
            border-radius: 5px;
            border: 2px solid #C0B283;
            font: 600 16px Montserrat, sans-serif;
            font-family: Montserrat, sans-serif;
        }
    </style>

    <style>
        body {
            font: 400 15px Montserrat, sans-serif;
            font-family: Montserrat, sans-serif;
            line-height: 1.8;
            color: #818181;
        }
        
        img {
            width: 100%;
            height: auto;
        }
        
        h2 {
            font-size: 24px;
            text-transform: uppercase;
            font-weight: 600;
            margin-bottom: 20px;
            text-align: center;
            font-family: Montserrat, sans-serif;
            letter-spacing: 4px;
        }
        
        h3 {
            font-size: 22px;
            font-weight: 400;
            margin-bottom: 20px;
            text-align: left;
        }
        
        h4 {
            font-size: 19px;
            line-height: 1.375em;
            font-weight: 400;
            margin-bottom: 20px;
            text-align: left;
        }
        
        h5 {
            font-size: 16px;
            line-height: 1.375em;
            font-weight: 400;
            margin-bottom: 15px;
        }
        
        h6 {
            font-size: 15px;
            line-height: 1.375em;
            font-weight: 400;
            margin-bottom: 14px;
            text-align: right;
        }
        
        .jumbotron {
            background-image: url("images/bulbs.jpg");
            background-size: 142% auto;
            background-align: center;
            padding: 100px 25px;
            font-family: Montserrat, sans-serif;
        }
        
        .container-fluid {
            padding: 60px 50px;
        }
        
        .section-navbar {
            color: #C0B283;
            background-color: #FFFFFF;
        }
        
        .section-default {
            color: #373737;
            background-color: #F4F4F4;
        }
        
        .section-about-me,
        .section-about-me h4,
        .section-about-me a {
            color: #373737;
            background-color: #F4F4F4;
            text-align: justify;
			font-size: 20px;
            line-height: 1.8em;
			
        }
        
        .section-recent-news {
            color: #373737;
            background-color: #ede6de;
        }
        
        .section-research {
            color: #373737;
            background-color: #F4F4F4;
            text-align: justify;
        }
        
        .section-research h3 {
            text-align: center;
        }
        
        .section-research h4 {
            text-align: center;
        }
        
        .section-publications,
        .section-publications a {
            color: #373737;
            background-color: #ede6de;
            text-align: justify;
        }
        
        .logo {
            color: #f4511e;
            font-size: 200px;
        }
        
        .thumbnail {
            padding: 0 0 15px 0;
            border: none;
            border-radius: 0;
        }
        
        .thumbnail2 {
            width: 60px;
            height: auto;
            margin-bottom: 10px;
        }
        
        .carousel-control.right,
        .carousel-control.left {
            background-image: none;
            color: #f4511e;
        }
        
        .carousel-indicators li {
            border-color: #f4511e;
        }
        
        .carousel-indicators li.active {
            background-color: #f4511e;
        }
        
        .item h4 {
            font-size: 19px;
            line-height: 1.375em;
            font-weight: 400;
            font-style: italic;
            margin: 70px 0;
        }
        
        .item span {
            font-style: normal;
        }
        
        .panel {
            border: 1px solid #f4511e;
            border-radius: 0 !important;
            transition: box-shadow 0.5s;
        }
        
        .panel:hover {
            box-shadow: 5px 0px 40px rgba(0, 0, 0, .2);
        }
        
        .panel-footer .btn:hover {
            border: 1px solid #f4511e;
            background-color: #fff !important;
            color: #f4511e;
        }
        
        .panel-heading {
            color: #fff !important;
            background-color: #f4511e !important;
            padding: 25px;
            border-bottom: 1px solid transparent;
            border-top-left-radius: 0px;
            border-top-right-radius: 0px;
            border-bottom-left-radius: 0px;
            border-bottom-right-radius: 0px;
        }
        
        .panel-footer {
            background-color: white !important;
        }
        
        .panel-footer h3 {
            font-size: 32px;
            font-weight: 600;
            margin-bottom: 30px;
        }
        
        .panel-footer h4 {
            font-size: 14px;
        }
        
        .panel-footer .btn {
            margin: 15px 0;
        }
        
        .navbar {
            margin-bottom: 0;
            z-index: 9999;
            border: 0;
            font-size: 12px !important;
            line-height: 1.42857143 !important;
            letter-spacing: 4px;
            border-radius: 0;
            font-family: Montserrat, sans-serif;
        }
        
        .navbar li a,
        .navbar .navbar-brand {
            font-family: Montserrat, sans-serif;
        }
        
        .navbar-nav li a:hover,
        .navbar-nav li.active a {
            background-color: #C0B283 !important;
            color: #FFFFFF !important;
            transition: 0.3s;
        }
        
        .navbar-default .navbar-toggle {
            border-color: transparent;
            color: #fff !important;
        }
        
        footer {
            background-color: #ffffff;
        }
        
        footer .glyphicon {
            font-size: 20px;
            margin-bottom: 20px;
            color: #C0B283;
        }
		
		.glyphicon {
            font-size: 20px;
            margin-bottom: 20px;
            color: #373737;
        }
        
        .slideanim {
            visibility: hidden;
        }
        
        .slide {
            animation-name: slide;
            -webkit-animation-name: slide;
            animation-duration: 1s;
            -webkit-animation-duration: 1s;
            visibility: visible;
        }
        
        @keyframes slide {
            0% {
                opacity: 0;
                transform: translateY(70%);
            }
            100% {
                opacity: 1;
                transform: translateY(0%);
            }
        }
        
        @-webkit-keyframes slide {
            0% {
                opacity: 0;
                -webkit-transform: translateY(70%);
            }
            100% {
                opacity: 1;
                -webkit-transform: translateY(0%);
            }
        }
        
        @media screen and (max-width: 768px) {
            .col-sm-4 {
                text-align: center;
                margin: 25px 0;
            }
            .btn-lg {
                width: 100%;
                margin-bottom: 35px;
            }
        }
        
        @media screen and (max-width: 480px) {
            .logo {
                font-size: 150px;
            }
        }
        
        .external-link-icon {
            float: left;
            margin: 0pt 5pt;
            width: 50pt;
            height: 50pt;
            opacity: 1;
        }
        
        .external-link-icon img {
            transition: 0.3s;
            border-radius: 50%;
        }
        
        .external-link-icon img:hover {
            background-color: #C0B283;
            transition: 0.3s;
            border-radius: 50%;
        }
        
        .external-link-container {
            text-align: center;
            width: 100%;
            height: 60pt;
        }
        
        .external-link-center-wrapper {
            margin: 20px auto auto;
            width: 300pt;
            height: 60pt;
        }
        
        .button {
            background-color: #373737;
            border: none;
            color: white;
            padding: 8px 32px;
            text-align: center;
            font-size: 18px;
            margin: 4px 2px;
            transition: 0.3s;
            display: inline-block;
            text-decoration: none;
            cursor: pointer;
            border-radius: 5px;
        }
        
        .button:hover {
            background-color: #C0B283;
            color: #ffffff;
        }
    </style>
</head>

<body id="myPage" data-spy="scroll" data-target=".navbar" data-offset="60" class="section-default">

    <nav class="navbar navbar-default navbar-fixed-top section-navbar">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#myPage">Tyler L. Hayes</a>
            </div>
            <div class="collapse navbar-collapse" id="myNavbar">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="#about">ABOUT</a></li>
                    <li><a href="#recent">NEWS</a></li>
                    <li><a href="#research">RESEARCH</a></li>
                    <li><a href="#publications">PUBLICATIONS</a></li>
                    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="data/Tyler_Hayes_CV.pdf" onclick="getOutboundLink('data/Tyler_Hayes_CV.pdf'); return false;" target="_blank">CV</a></li>
                    </h1>
            </div>
        </div>
    </nav>

    <div class="jumbotron text-center">
        <h1>Tyler L. Hayes<br>
  	<div class = "external-link-container">
	  <div class="external-link-center-wrapper">
	    <div class = "external-link-icon">
	    	<a href="https://www.linkedin.com/in/tyler-hayes-6a91a461/" onclick="getOutboundLink('https://www.linkedin.com/in/tyler-hayes-6a91a461/'); return false;" target="_blank">
            	<span><img border="0" alt="Tyler Hayes Linkedin" src="images/linkedin-logo.png"></img></span>
            </a>
	    </div>
	    <div class = "external-link-icon">
	    	<a href="https://scholar.google.com/citations?user=LZ1PWfcAAAAJ&hl=en#" onclick="getOutboundLink('https://scholar.google.com/citations?user=LZ1PWfcAAAAJ&hl=en#'); return false;" target="_blank">
            	<span><img border="0" alt="Tyler Hayes Google Scholar" src="images/gs-logo.png"></img></span>
            </a>
	    </div>
		<div class = "external-link-icon">
	    	<a href="https://github.com/tyler-hayes" onclick="getOutboundLink('https://github.com/tyler-hayes'); return false;" target="_blank">
				<span><img border="0" alt="Tyler Hayes GitHub" src="images/github-logo.png"></img></span>
            </a>
	    </div>
		<div class = "external-link-icon">
	    	<a href="https://twitter.com/tylerlhayes" onclick="getOutboundLink('https://twitter.com/tylerlhayes'); return false;" target="_blank">
				<span><img border="0" alt="Tyler Hayes Twitter" src="images/twitter-logo.png"></img></span>
            </a>
	    </div>
	    <div class = "external-link-icon">
	    	<a href="" rel="nofollow" onclick="this.href='mailto:' + 'tlh6792' + '@' + 'rit.edu'" target="_blank">
            	<span><img border="0" alt="Tyler Hayes Contact" src="images/mail-logo.png"></img></span>
            </a>
	    </div>
	  </div>
	</div>
</div>

  </form>

<!-- Container (About Section) -->
<div id="about" class="container-fluid section-about-me">
  <div class="row">
    <h2>About Me</h2>
    <div class="col-sm-2 col-sm-offset-3">
      <span><img src="images/tyler.jpg" alt="Tyler Hayes" class="img-responsive" style="border-radius: 25px"></span>
	</div>
    <div class="col-sm-4">
      <h4>Hello! I recently defended my PhD in the <a href="https://www.cis.rit.edu/" onclick="getOutboundLink('https://www.cis.rit.edu/'); return false;">Chester F. Carlson Center for Imaging Science</a> at the <a href="https://www.rit.edu/" onclick="getOutboundLink('https://www.rit.edu/'); return false;">Rochester Institute of Technology (RIT)</a> in Rochester, NY. At RIT, I worked in the <a href="http://klab.cis.rit.edu/" onclick="getOutboundLink('http://klab.cis.rit.edu/'); return false;"> Machine and Neuromorphic Perception Laboratory (a.k.a. kLab)</a> under the direction of my advisor, <a href="https://chriskanan.com/" onclick="getOutboundLink('https://chriskanan.com/'); return false;" >Dr. Christopher Kanan</a>. My current research interests include lifelong machine learning, computer vision, and computational mathematics. I am a current board member of the ContinualAI non-profit organization.
I spent Summer 2021 as a Research Intern at <a href="https://ai.facebook.com/" onclick="getOutboundLink('https://ai.facebook.com/'); return false;">Facebook AI Research (FAIR)</a> working with Dr. Arthur Szlam and Dr. Ludovic Denoyer. Previously, I earned a BS in Applied Mathematics from RIT in 2014 and an MS in Applied and Computational Mathematics from RIT in 2017. I have an <a href="https://www.csauthors.net/distance/tyler-l-hayes/paul-erdos" onclick="getOutboundLink('https://www.csauthors.net/distance/tyler-l-hayes/paul-erdos'); return false;">Erdős number</a> of 3!
	  </h4>
	  
		<!--<h3><p style="color:#6305DC; text-align: justify; font-weight: bold; line-height: 1.5em">I am currently on the job market for research positions related to lifelong continual machine learning. Please feel free to reach out to me via email with any relevant roles.</p>
	  </h3> -->
	  
    </div>
  </div>
</div>

<!-- Container (News) -->
<div id="recent" class="container-fluid section-recent-news">
  <div class="row">
    <div class="col-sm-8 col-sm-offset-2">
      <h2>News</h2>
	  
		<!-- Recent News -->
		<!-- <h4><strong>Sep 2020:</strong> <span class="newflag">NEW!</span> Our paper <a href="https://doi.org/10.1371/journal.pone.0238302" onclick="getOutboundLink('https://doi.org/10.1371/journal.pone.0238302'); return false;">"Are Open Set Classification Methods Effective on Large-Scale Datasets?"</a> was published in PLoS ONE!</h4>-->
		<h4><strong>Jun 2022:</strong> <span class="newflag">NEW!</span>  Served on a panel and gave an invited talk at the <a href="https://sites.google.com/view/clvision2022" onclick="getOutboundLink('https://sites.google.com/view/clvision2022'); return false;">CLVISION Workshop</a> at CVPR 2022 on Real-World Applications of Continual Learning</a>!</h4>
		<h4><strong>May 2022:</strong> Our paper <a href="https://arxiv.org/abs/2203.10681" onclick="getOutboundLink('https://arxiv.org/abs/2203.10681'); return false;">"Online Continual Learning for Embedded Devices"</a> was accepted for poster presentation at CoLLAs 2022!</h4>
		<h4><strong>Mar 2022:</strong> Joined the board of the <a href="https://www.continualai.org/" onclick="getOutboundLink('https://www.continualai.org/'); return false;">Continual AI</a> non-profit organization!</h4>
		<h4><strong>Mar 2022:</strong> Successfully defended my doctoral dissertation!</h4>
		<h4><strong>Dec 2021:</strong> Our paper <a href="https://arxiv.org/abs/2107.05445" onclick="getOutboundLink('https://arxiv.org/abs/2107.05445'); return false;">"Disentangling Transfer and Interference in Multi-Domain Learning"</a> was accepted to the AAAI 2022 Workshop on Practical Deep Learning in the Wild!</h4>
		<h4><strong>Oct 2021:</strong> Our paper <a href="https://arxiv.org/abs/2103.14010" onclick="getOutboundLink('https://arxiv.org/abs/2103.14010'); return false;">"Self-Supervised Training Enhances Online Continual Learning"</a> was accepted for poster presentation at BMVC 2021! (36.2% acceptance rate)</h4>
		<h4><strong>Jun 2021:</strong> Gave an invited talk at the <a href="https://www.continualai.org/" onclick="getOutboundLink('https://www.continualai.org/'); return false;">Continual AI</a> Reading Group on our paper <a href="https://youtu.be/iCV7_PD_uBQ" onclick="getOutboundLink('https://youtu.be/iCV7_PD_uBQ'); return false;">"Replay in Deep Learning: Current Approaches and Missing Biological
  Elements"</a>!</h4>
		<h4><strong>May 2021:</strong> Our paper <a href="https://arxiv.org/abs/2104.04132" onclick="getOutboundLink('https://arxiv.org/abs/2104.04132'); return false;">"Replay in Deep Learning: Current Approaches and Missing Biological
  Elements"</a> was accepted for publication in the MIT Press journal of Neural Computation!</h4>
		<h4><strong>May 2021:</strong> Started working as a Research Intern at <a href="https://ai.facebook.com/" onclick="getOutboundLink('https://ai.facebook.com/'); return false;">Facebook AI Research (FAIR)</a>!</h4>
		<h4><strong>Apr 2021:</strong> Our paper <a href="https://arxiv.org/abs/2103.03987" onclick="getOutboundLink('https://arxiv.org/abs/2103.03987'); return false;">"Selective Replay Enhances Learning in Online Continual Analogical Reasoning"</a> was accepted for oral presentation at the CVPR 2021 Workshop on Continual Learning! An extended abstract of the paper was also accepted to the CVPR 2021 Workshop on Women in Computer Vision (WiCV)!</h4>
			
		
    </div>
	
	<!-- Collapsed News -->
	<a class="col-sm-8 col-sm-offset-2 text-center" data-toggle="collapse" data-target="#collapseNews" role="button" aria-expanded="false" aria-controls="collapseNews" ><span class="glyphicon glyphicon-chevron-down"></span></a>
	<div class="col-sm-8 col-sm-offset-2 collapse" id="collapseNews">
		<h4><strong>Nov 2020:</strong> Gave an invited talk at the <a href="https://www.continualai.org/" onclick="getOutboundLink('https://www.continualai.org/'); return false;">Continual AI</a> Meetup on <a href="https://youtu.be/MXJO9F-ohe8?t=1931" onclick="getOutboundLink('https://youtu.be/MXJO9F-ohe8?t=1931'); return false;">Benchmarks and Evaluation for Continual Learning</a>!</h4>
		<h4><strong>Sep 2020:</strong> Our paper <a href="https://doi.org/10.1371/journal.pone.0238302" onclick="getOutboundLink('https://doi.org/10.1371/journal.pone.0238302'); return false;">"Are Open Set Classification Methods Effective on Large-Scale Datasets?"</a> was published in PLoS ONE!</h4>
		<h4><strong>Aug 2020:</strong> Successfully defended my dissertation proposal and advanced to candidacy.</h4>
		<h4><strong>Jul 2020:</strong> Our paper <a href="https://arxiv.org/abs/2008.06439" onclick="getOutboundLink('https://arxiv.org/abs/2008.06439'); return false;">"RODEO: Replay for Online Object Detection"</a> was accepted for poster presentation at BMVC 2020! (29.1% acceptance rate)</h4>
		<h4><strong>Jul 2020:</strong> Our paper <a href="https://arxiv.org/abs/2009.04659" onclick="getOutboundLink('https://arxiv.org/abs/2009.04659'); return false;">"Improved Robustness to Open Set Inputs via Tempered Mixup"</a> was accepted to the ECCV 2020 Workshop on Adversarial Robustness in the Real World!</h4>
		<h4><strong>Jul 2020:</strong> Our paper <a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;">"REMIND Your Neural Network to Prevent Catastrophic Forgetting"</a> was accepted for poster presentation at ECCV 2020! (27.1% acceptance rate)</h4>
		<h4><strong>Jun 2020:</strong> Our paper on Deep SLDA won the <a href="https://sites.google.com/view/clvision2020/paper-awards?authuser=0" onclick="getOutboundLink('https://sites.google.com/view/clvision2020/paper-awards?authuser=0'); return false;">Best Paper Award</a> at the CVPR 2020 Workshop on Continual Learning!</h4>
		<h4><strong>May 2020:</strong> Gave an invited talk at the <a href="https://www.continualai.org/" onclick="getOutboundLink('https://www.continualai.org/'); return false;">Continual AI</a> Meetup on <a href="https://youtu.be/Qo2JKIDZz6w?t=2302" onclick="getOutboundLink('https://youtu.be/Qo2JKIDZz6w?t=2302'); return false;">Continual Learning with Sequential Streaming Data</a>!</h4>
		<h4><strong>May 2020:</strong> Won a travel grant to attend the CVPR Workshop on Women in Computer Vision (WiCV)!</h4>
		<h4><strong>Apr 2020:</strong> My research journey was featured in an <a href="https://www.rit.edu/science/news/student-student-artificial-intelligencemachine-learning?fbclid=IwAR1SMfMZA6ryy8C31YATz5CHOOfDnybFgdnfnfyMO74wYGXod7pqXzJcE9I" onclick="getOutboundLink('https://www.rit.edu/science/news/student-student-artificial-intelligencemachine-learning?fbclid=IwAR1SMfMZA6ryy8C31YATz5CHOOfDnybFgdnfnfyMO74wYGXod7pqXzJcE9I'); return false;">RIT News article</a>!</h4>
		<h4><strong>Apr 2020:</strong> Our extended abstract <a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;">"REMIND Your Neural Network to Prevent Catastrophic Forgetting"</a> was accepted to the CVPR 2020 Workshop on Women in Computer Vision (WiCV)! </h4>
		<h4><strong>Apr 2020:</strong> Our paper <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html'); return false;">"Stream-51: Streaming Classification and Novelty Detection from Videos"</a> was accepted for poster presentation at the CVPR 2020 Workshop on Continual Learning! See our project webpage <a href="https://tyler-hayes.github.io/stream51" onclick="getOutboundLink('https://tyler-hayes.github.io/stream51'); return false;">here</a>.</h4>
		<h4><strong>Apr 2020:</strong> Our paper <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html'); return false;">"Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis"</a> was accepted for oral presentation at the CVPR 2020 Workshop on Continual Learning!</h4>
		<h4><strong>Apr 2019:</strong> Gave an invited talk at the RIT <a href="https://www.rit.edu/chai/" onclick="getOutboundLink('https://www.rit.edu/chai/'); return false;">CHAI</a> AI Seminar Series on "Memory Efficient Experience Replay for Mitigating Catastrophic Forgetting."</h4>	
		<h4><strong>Jan 2019:</strong> Our paper <a href="https://ieeexplore.ieee.org/document/8793982" onclick="getOutboundLink('https://ieeexplore.ieee.org/document/8793982'); return false;">"Memory Efficient Experience Replay for Streaming Learning"</a> was accepted for poster presentation at ICRA 2019! (44.0% acceptance rate)</h4>
		<h4><strong>Feb 2018:</strong> Our paper <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html'); return false;">"Compassionately Conservative Balanced Cuts for Image Segmentation"</a> was accepted for poster presentation at CVPR 2018! (29.6% acceptance rate)</h4>
		<h4><strong>Nov 2017:</strong> Our paper <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410" onclick="getOutboundLink('https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410'); return false;">"Measuring Catastrophic Forgetting in Neural Networks"</a> was accepted for a spotlight presentation at AAAI 2018! (24.6% acceptance rate)</h4>
		<h4><strong>Jun 2017:</strong> Passed my PhD Qualification Exam.</h4>		   
		<h4><strong>Jun 2017:</strong> Started working as a Research Intern in the <a href="https://www.nrl.navy.mil/itd/aic/" onclick="getOutboundLink('https://www.nrl.navy.mil/itd/aic/'); return false;">Navy Center for Applied Research in Artificial Intelligence</a> at the <a href="https://www.nrl.navy.mil/" onclick="getOutboundLink('https://www.nrl.navy.mil/'); return false;">US Naval Research Laboratory (NRL)</a>.</h4>
		<h4><strong>Mar 2017:</strong> Successfully defended my MS thesis.</h4>
    </div>
  </div>
</div>

<!-- Container (Research Section) -->
<div id="research" class="container-fluid section-research">
	<div class="row">
		<h2>Research</h2>
	</div>
	
	<!-- CoLLAs 2022 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CoLLAs 2022</strong>: Online Continual Learning for Embedded Devices</h3>
			<h4><strong>Tyler L. Hayes</strong> & Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2203.10681" onclick="getOutboundLink('https://arxiv.org/abs/2203.10681'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://github.com/tyler-hayes/Embedded-CL" onclick="getOutboundLink('https://github.com/tyler-hayes/Embedded-CL'); return false;" target="_blank"><button class="button">Code</button></a>
			</h3>
		</div>
	</div>
	
	<div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/openloris_bar.png" alt="arXiv 2022" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Real-time on-device continual learning is needed for new applications such as home robots, user personalization on smartphones, and augmented/virtual reality headsets. However, this setting poses unique challenges: embedded devices have limited memory and compute capacity and conventional machine learning models suffer from catastrophic forgetting when updated on non-stationary data streams. While several online continual learning models have been developed, their effectiveness for embedded applications has not been rigorously studied. In this paper, we first identify criteria that online continual learners must meet to effectively perform real-time, on-device learning. We then study the efficacy of several online continual learning methods when used with mobile neural networks. We measure their performance, memory usage, compute requirements, and ability to generalize to out-of-domain inputs.</p></h5>
		</div>
    </div>

	<!-- arXiv 2022 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>arXiv 2022</strong>: Can I see an Example? Active Learning the Long Tail of Attributes and Relations</h3>
			<h4><strong>Tyler L. Hayes</strong>, Maximilian Nickel, Christopher Kanan, Ludovic Denoyer, Arthur Szlam</h4>
			<h3>
				<a href="http://arxiv.org/abs/2203.06215" onclick="getOutboundLink('http://arxiv.org/abs/2203.06215'); return false;" target="_blank"><button class="button">arXiv</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/active-learn.png" alt="arXiv 2022" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>There has been significant progress in creating machine learning models that identify objects in scenes along with their associated attributes and relationships; however, there is a large gap between the best models and human capabilities. One of the major reasons for this gap is the difficulty in collecting sufficient amounts of annotated relations and attributes for training these systems. While some attributes and relations are abundant, the distribution in the natural world and existing datasets is long tailed. In this paper, we address this problem by introducing a novel incremental active learning framework that asks for attributes and relations in visual scenes. While conventional active learning methods ask for labels of specific examples, we flip this framing to allow agents to ask for examples from specific categories. Using this framing, we introduce an active sampling method that asks for examples from the tail of the data distribution and show that it outperforms classical active learning methods on Visual Genome.</p></h5>
		</div>
    </div>

	<!-- AAAIW 2022 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>AAAIW 2022</strong>: Disentangling Transfer and Interference in Multi-Domain Learning</h3>
			<h4>Yipeng Zhang, <strong>Tyler L. Hayes</strong>, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2107.05445" onclick="getOutboundLink('https://arxiv.org/abs/2107.05445'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://practical-dl.github.io/long_paper/05.pdf" onclick="getOutboundLink('https://practical-dl.github.io/long_paper/05.pdf'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/transfer-interference.png" alt="arXiv 2021" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Humans are incredibly good at transferring knowledge from one domain to another, enabling rapid learning of new tasks. Likewise, transfer learning has enabled enormous success in many computer vision problems using pretraining. However, the benefits of transfer in multi-domain learning, where a network learns multiple tasks defined by different datasets, has not been adequately studied. Learning multiple domains could be beneficial or these domains could interfere with each other given limited network capacity. In this work, we decipher the conditions where interference and knowledge transfer occur in multi-domain learning. We propose new metrics disentangling interference and transfer and set up experimental protocols. We further examine the roles of network capacity, task grouping, and dynamic loss weighting in reducing interference and facilitating transfer. We demonstrate our findings on the CIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.</p></h5>
		</div>
    </div>
	
	<!-- Neural Computation 2021 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>Neural Computation 2021</strong>: Replay in Deep Learning: Current Approaches and Missing Biological Elements</h3>
			<h4><strong>Tyler L. Hayes</strong>, Giri P. Krishnan, Maxim Bazhenov, Hava T. Siegelmann, Terrence J. Sejnowski, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2104.04132" onclick="getOutboundLink('https://arxiv.org/abs/2104.04132'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://doi.org/10.1162/neco_a_01433" onclick="getOutboundLink('https://doi.org/10.1162/neco_a_01433'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://youtu.be/iCV7_PD_uBQ" onclick="getOutboundLink('https://youtu.be/iCV7_PD_uBQ'); return false;" target="_blank"><button class="button">Video</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/replay.png" alt="arXiv 2021" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Replay is the reactivation of one or more neural patterns, which are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated into deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this paper, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be utilized to improve artificial neural networks.</p></h5>
		</div>
    </div>
	
	<!-- BMVC 2021 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>BMVC 2021</strong>: Self-Supervised Training Enhances Online Continual Learning</h3>
			<h4>Jhair Gallardo, <strong>Tyler L. Hayes</strong>, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2103.14010" onclick="getOutboundLink('https://arxiv.org/abs/2103.14010'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://www.bmvc2021-virtualconference.com/assets/papers/0636.pdf" onclick="getOutboundLink('https://www.bmvc2021-virtualconference.com/assets/papers/0636.pdf'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/self-supervised-cl.png" alt="arXiv 2021" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>In continual learning, a system must incrementally learn from a non-stationary data stream without catastrophic forgetting. Recently, multiple methods have been devised for incrementally learning classes on large-scale image classification tasks, such as ImageNet. State-of-the-art continual learning methods use an initial supervised pre-training phase, in which the first 10% - 50% of the classes in a dataset are used to learn representations in an offline manner before continual learning of new classes begins. We hypothesize that self-supervised pre-training could yield features that generalize better than supervised learning, especially when the number of samples used for pre-training is small. We test this hypothesis using the self-supervised MoCo-V2 and SwAV algorithms. On ImageNet, we find that both outperform supervised pre-training considerably for online continual learning, and the gains are larger when fewer samples are available. Our findings are consistent across three continual learning algorithms. Our best system achieves a 14.95% relative increase in top-1 accuracy on class incremental ImageNet over the prior state of the art for online continual learning.</p></h5>
		</div>
    </div>
	
	
	<!-- CVPRW 2021 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2021</strong>: Selective Replay Enhances Learning in Online Continual Analogical Reasoning</h3>
			<h4><strong>Tyler L. Hayes</strong> & Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2103.03987" onclick="getOutboundLink('https://arxiv.org/abs/2103.03987'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Hayes_Selective_Replay_Enhances_Learning_in_Online_Continual_Analogical_Reasoning_CVPRW_2021_paper.pdf" onclick="getOutboundLink('https://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Hayes_Selective_Replay_Enhances_Learning_in_Online_Continual_Analogical_Reasoning_CVPRW_2021_paper.pdf'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/tyler-hayes/Continual-Analogical-Reasoning" onclick="getOutboundLink('https://github.com/tyler-hayes/Continual-Analogical-Reasoning'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="https://youtu.be/NeqPhripSkI?t=11625" onclick="getOutboundLink('https://youtu.be/NeqPhripSkI?t=11625'); return false;" target="_blank"><button class="button">Oral</button></a>
				<a href="https://youtu.be/lTfpzrpCme4" onclick="getOutboundLink('https://youtu.be/lTfpzrpCme4'); return false;" target="_blank"><button class="button">Video</button></a>
				<a href="data/cvpr_workshop_2021.pdf" onclick="getOutboundLink('data/cvpr_workshop_2021.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/arxiv_2021.png" alt="arXiv 2021" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>In continual learning, a system learns from non-stationary data streams or batches without catastrophic forgetting. While this problem has been heavily studied in supervised image classification and reinforcement learning, continual learning in neural networks designed for abstract reasoning has not yet been studied. Here, we study continual learning of analogical reasoning. Analogical reasoning tests such as Raven's Progressive Matrices (RPMs) are commonly used to measure non-verbal abstract reasoning in humans, and recently offline neural networks for the RPM problem have been proposed. In this paper, we establish experimental baselines, protocols, and forward and backward transfer metrics to evaluate continual learners on RPMs. We employ experience replay to mitigate catastrophic forgetting. Prior work using replay for image classification tasks has found that selectively choosing the samples to replay offers little, if any, benefit over random selection. In contrast, we find that selective replay can significantly outperform random selection for the RPM task.</p></h5>
		</div>
    </div>
	
	<!-- PLOS ONE 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>PLoS ONE 2020</strong>: Are Open Set Classification Methods Effective on Large-Scale Datasets?</h3>
			<h4>Ryne Roady, <strong>Tyler L. Hayes</strong>, Ronald Kemker, Ayesha Gonzales, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1910.14034" onclick="getOutboundLink('https://arxiv.org/abs/1910.14034'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://doi.org/10.1371/journal.pone.0238302" onclick="getOutboundLink('https://doi.org/10.1371/journal.pone.0238302'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/arxiv_2019_ood.PNG" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Supervised classification methods often assume the train and test data distributions are the same and that all classes in the test set are present in the training set. However, deployed classifiers often require the ability to recognize inputs from outside the training set as unknowns. This problem has been studied under multiple paradigms including out-of-distribution detection and open set recognition. For convolutional neural networks, there have been two major approaches: 1) inference methods to separate knowns from unknowns and 2) feature space regularization strategies to improve model robustness to novel inputs. Up to this point, there has been little attention to exploring the relationship between the two approaches and directly comparing performance on large-scale datasets that have more than a few dozen categories. Using the ImageNet ILSVRC-2012 large-scale classification dataset, we identify novel combinations of regularization and specialized inference methods that perform best across multiple open set classification problems of increasing difficulty level. We find that input perturbation and temperature scaling yield significantly better performance on large-scale datasets than other inference methods tested, regardless of the feature space regularization strategy. Conversely, we find that improving performance with advanced regularization schemes during training yields better performance when baseline inference techniques are used; however, when advanced inference methods are used to detect open set classes, the utility of these combersome training paradigms is less evident.</p></h5>
		</div>
    </div>
	
	<!-- BMVC 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>BMVC 2020</strong>: RODEO: Replay for Online Object Detection</h3>
			<h4>Manoj Acharya, <strong>Tyler L. Hayes</strong>, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2008.06439" onclick="getOutboundLink('https://arxiv.org/abs/2008.06439'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://www.bmvc2020-conference.com/conference/papers/paper_0526.html" onclick="getOutboundLink('https://www.bmvc2020-conference.com/conference/papers/paper_0526.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/manoja328/rodeo" onclick="getOutboundLink('https://github.com/manoja328/rodeo'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="https://www.youtube.com/watch?v=DHzpb4oiKfA" onclick="getOutboundLink('https://www.youtube.com/watch?v=DHzpb4oiKfA'); return false;" target="_blank"><button class="button">Video</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/rodeo.png" alt="BMVC 2020" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Humans can incrementally learn to do new visual detection tasks, which is a huge challenge for today's computer vision systems. Incrementally trained deep learning models lack backwards transfer to previously seen classes and suffer from a phenomenon known as catastrophic forgetting. In this paper, we pioneer online streaming learning for object detection, where an agent must learn examples one at a time with severe memory and computational constraints. In object detection, a system must output all bounding boxes for an image with the correct label. Unlike earlier work, the system described in this paper can learn how to do this task in an online manner with new classes being introduced over time. We achieve this capability by using a novel memory replay mechanism that replays entire scenes in an efficient manner. We achieve state-of-the-art results on both the PASCAL VOC 2007 and MS COCO datasets.</p></h5>
		</div>
    </div>

	<!-- ECCVW 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>ECCVW 2020</strong>: Improved Robustness to Open Set Inputs via Tempered Mixup</h3>
			<h4>Ryne Roady, <strong>Tyler L. Hayes</strong>, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/2009.04659" onclick="getOutboundLink('https://arxiv.org/abs/2009.04659'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://link.springer.com/chapter/10.1007/978-3-030-66415-2_12" onclick="getOutboundLink('https://link.springer.com/chapter/10.1007/978-3-030-66415-2_12'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/t-mixup.png" alt="ECCVW 2020" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Supervised classification methods often assume that evaluation data is drawn from the same distribution as training data and that all classes are present for training. However, real-world classifiers must handle inputs that are far from the training distribution including samples from unknown classes. Open set robustness refers to the ability to properly label samples from previously unseen categories as novel and avoid high-confidence, incorrect predictions. Existing approaches have focused on either novel inference methods, unique training architectures, or supplementing the training data with additional background samples. Here, we propose a simple regularization technique easily applied to existing CNN architectures that improves open set robustness without a background dataset. Our method achieves state-of-the-art results on open set classification baselines and easily scales to large-scale open set classification problems.</p></h5>
		</div>
    </div>

	<!-- ECCV 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>ECCV 2020</strong>: REMIND Your Neural Network to Prevent Catastrophic Forgetting</h3>
			<h4><strong>Tyler L. Hayes*</strong>, Kushal Kafle*, Robik Shrestha*, Manoj Acharya, Christopher Kanan</h4>
			<h6>* denotes equal contribution.</h6>
			<h3>
				<a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58598-3_28" onclick="getOutboundLink('https://link.springer.com/chapter/10.1007%2F978-3-030-58598-3_28'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/tyler-hayes/REMIND" onclick="getOutboundLink('https://github.com/tyler-hayes/REMIND'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="https://youtu.be/Qo2JKIDZz6w?t=2302" onclick="getOutboundLink('https://youtu.be/Qo2JKIDZz6w?t=2302'); return false;" target="_blank"><button class="button">Continual AI Talk</button></a>
				<a href="https://www.youtube.com/watch?v=rg55zzj5_PM&t=4s" onclick="getOutboundLink('https://www.youtube.com/watch?v=rg55zzj5_PM&t=4s'); return false;" target="_blank"><button class="button">10 Min Video</button></a>
				<a href="https://www.youtube.com/watch?v=xA5UMOdu8y8" onclick="getOutboundLink('https://www.youtube.com/watch?v=xA5UMOdu8y8'); return false;" target="_blank"><button class="button">1 Min Video</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/arxiv_2019_remind.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While there is neuroscientific evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND's generality by pioneering online learning for Visual Question Answering (VQA).</p></h5>
		</div>
    </div>

	<!-- CVPRW 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2020</strong>: Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis</h3>
			<h4><strong>Best Paper Award at the CVPR 2020 Workshop on Continual Learning in Computer Vision</strong></h4>
			<h4><strong>Tyler L. Hayes</strong> & Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1909.01520" onclick="getOutboundLink('https://arxiv.org/abs/1909.01520'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/tyler-hayes/Deep_SLDA" onclick="getOutboundLink('https://github.com/tyler-hayes/Deep_SLDA'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="data/Deep_SLDA_Poster.pdf" onclick="getOutboundLink('data/Deep_SLDA_Poster.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
				<a href="https://www.youtube.com/watch?v=Nklyx-AElwo" onclick="getOutboundLink('https://www.youtube.com/watch?v=Nklyx-AElwo'); return false;" target="_blank"><button class="button">Video</button></a>
			</h3>
		</div>
	</div>
	
    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/cvprw_2020_slda.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated, which prohibits many applications. An alternative paradigm is online learning in a single pass through the training dataset on a resource constrained budget, which is known as streaming learning. Streaming learning has been much less studied in the deep learning community. In streaming learning, an agent learns instances one-by-one and can be tested at any time, rather than only after learning a large batch. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet ILSVRC-2012 and CORe50, a dataset that involves learning to classify from temporally ordered samples.</p></h5>
		</div>
    </div>
	
	<!-- CVPRW 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2020</strong>: Stream-51: Streaming Classification and Novelty Detection from Videos</h3>
			<h4>Ryne Roady*, <strong>Tyler L. Hayes*</strong>, Hitesh Vaidya, Christopher Kanan</h4>
			<h6>* denotes equal contribution.</h6>
			<h3>
				<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://tyler-hayes.github.io/stream51" onclick="getOutboundLink('https://tyler-hayes.github.io/stream51'); return false;" target="_blank"><button class="button">Project Page</button></a>
				<a href="https://github.com/tyler-hayes/Stream-51" onclick="getOutboundLink('https://github.com/tyler-hayes/Stream-51'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="https://youtu.be/MXJO9F-ohe8?t=1931" onclick="getOutboundLink('https://youtu.be/MXJO9F-ohe8?t=1931'); return false;" target="_blank"><button class="button">Continual AI Talk</button></a>
				<a href="https://www.youtube.com/watch?v=cRtRTcWqhsU" onclick="getOutboundLink('https://www.youtube.com/watch?v=cRtRTcWqhsU'); return false;" target="_blank"><button class="button">1 Min Video</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/cvprw_2020_stream51.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Deep neural networks are popular for visual perception tasks such as image classification and object detection. Once trained and deployed in a real-time environment, these models struggle to identify novel inputs not initially represented in the training distribution. Further, they cannot be easily updated on new information or they will catastrophically forget previously learned knowledge. While there has been much interest in developing models capable of overcoming forgetting, most research has focused on incrementally learning from common image classification datasets broken up into large batches. Online streaming learning is a more realistic paradigm where a model must learn one sample at a time from temporally correlated data streams. Although there are a few datasets designed specifically for this protocol, most have limitations such as few classes or poor image quality. In this work, we introduce Stream-51, a new dataset for streaming classification consisting of temporally correlated images from 51 distinct object categories and additional evaluation classes outside of the training distribution to test novelty recognition. We establish unique evaluation protocols, experimental metrics, and baselines for our dataset in the streaming paradigm.</p></h5>
		</div>
    </div>
	
	<!-- arXiv Preprint 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>arXiv 2020</strong>: Do We Need Fully Connected Output Layers in Convolutional Networks?</h3>
			<h4>Zhongchao Qian, <strong>Tyler L. Hayes</strong>, Kushal Kafle, Christopher Kanan</h4>
			<h3>
				<a href="http://arxiv.org/abs/2004.13587" onclick="getOutboundLink('http://arxiv.org/abs/2004.13587'); return false;" target="_blank"><button class="button">arXiv</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/arxiv_2019_fixed_output.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Traditionally, deep convolutional neural networks consist of a series of convolutional and pooling layers followed by one or more fully connected (FC) layers to perform the final classification. While this design has been successful, for datasets with a large number of categories, the fully connected layers often account for a large percentage of the network's parameters. For applications with memory constraints, such as mobile devices and embedded platforms, this is not ideal. Recently, a family of architectures that involve replacing the learned fully connected output layer with a fixed layer has been proposed as a way to achieve better efficiency. In this paper we examine this idea further and demonstrate that fixed classifiers offer no additional benefit compared to simply removing the output layer along with its parameters. We further demonstrate that the typical approach of having a fully connected final output layer is inefficient in terms of parameter count. We are able to achieve comparable performance to a traditionally learned fully connected classification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196, and Oxford Flowers-102 datasets, while not having a fully connected output layer at all.</p></h5>
		</div>
    </div>

    <!-- ICRA 2019 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>ICRA 2019</strong>: Memory Efficient Experience Replay for Streaming Learning</h3>
			<h4><strong>Tyler L. Hayes</strong>, Nathan D. Cahill, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1809.05922" onclick="getOutboundLink('https://arxiv.org/abs/1809.05922'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://ieeexplore.ieee.org/document/8793982" onclick="getOutboundLink('https://ieeexplore.ieee.org/document/8793982'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/tyler-hayes/ExStream" onclick="getOutboundLink('https://github.com/tyler-hayes/ExStream'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="data/ICRA2019_Hayes_Poster.pdf" onclick="getOutboundLink('data/ICRA2019_Hayes_Poster.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/icra_2019.png" alt="ICRA 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.</p></h5>
		</div>
    </div>

	<!-- CVPRW 2018 -->
			</ul></h5>
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2018</strong>: New Metrics and Experimental Paradigms for Continual Learning</h3>
			<h4><strong>Tyler L. Hayes</strong>, Ronald Kemker, Nathan D. Cahill, Christopher Kanan</h4>
			<h3>
				<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="data/cvpr18_workshop_poster_final.pdf" onclick="getOutboundLink('data/cvpr18_workshop_poster_final.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/cvprw_2018.png" alt="CVPRW 2018" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>In order for a robotic agent to learn successfully in an uncontrolled environment, it must be able to immediately alter its behavior. Deep neural networks are the dominant approach for classification tasks in computer vision, but typical algorithms and architectures are incapable of immediately learning new tasks without catastrophically forgetting previously acquired knowledge. There has been renewed interest in solving this problem, but there are limitations to existing solutions, including poor performance compared to offline models, large memory footprints, and learning slowly. In this abstract, we formalize the continual learning paradigm and propose new benchmarks for assessing continual learning agents.
</p></h5>
		</div>
    </div>

	<!-- CVPR 2018 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPR 2018</strong>: Compassionately Conservative Balanced Cuts for Image Segmentation</h3>
			<h4>Nathan D. Cahill, <strong>Tyler L. Hayes</strong>, Renee T. Meinhold, John F. Hamilton</h4>
			<h3>
				<a href="https://arxiv.org/abs/1803.09903" onclick="getOutboundLink('https://arxiv.org/abs/1803.09903'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/cvpr_2018.PNG" alt="CVPR 2018" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the Buhler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained lτ -minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.</p></h5>
		</div>
    </div>

     <!-- AAAI 2018 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>AAAI 2018</strong>: Measuring Catastrophic Forgetting in Neural Networks</h3>
			<h4>Ronald Kemker, Angelina Abitino, Marc McClure, <strong>Tyler L. Hayes</strong>, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1708.02072" onclick="getOutboundLink('https://arxiv.org/abs/1708.02072'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410" onclick="getOutboundLink('https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-2 col-sm-offset-2">
			<img src="images/aaai_2018.png" alt="AAAI 2018" class="img-responsive"></img>
		</div>
		<div class="col-sm-6">
			<h5 class="h5-small"><p>Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than retraining the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.</p></h5>
		</div>
    </div>
</div>

<!-- Container (Publications) -->
<div id="publications" class="container-fluid section-publications">
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h2>Publications</h2>
			<h4>Peer-Reviewed Papers</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes</strong> and C. Kanan. <a href="https://arxiv.org/abs/2203.10681" onclick="getOutboundLink('https://arxiv.org/abs/2203.10681'); return false;">Online continual learning for embedded devices</a>. In: Conference on Lifelong Learning Agents (CoLLAs), 2022</li>
				<li>Y. Zhang, <strong>T.L. Hayes</strong>, and C. Kanan. <a href="https://arxiv.org/abs/2107.05445" onclick="getOutboundLink('https://arxiv.org/abs/2107.05445'); return false;">Disentangling transfer and interference in multi-domain learning</a>. AAAI Workshop: Practical Deep Learning in the Wild, 2022</li>
				<li>J. Gallardo, <strong>T.L. Hayes</strong>, and C. Kanan. <a href="https://arxiv.org/abs/2103.14010" onclick="getOutboundLink('https://arxiv.org/abs/2103.14010'); return false;">Self-supervised training enhances online continual learning</a>. In: British Machine Vision Conference (BMVC), 2021</li>
				<li><strong>T.L. Hayes</strong>, G.P. Krishnan, M. Bazhenov, H.T. Siegelmann, T.J. Sejnowski, and C. Kanan. <a href="https://arxiv.org/abs/2104.04132" onclick="getOutboundLink('https://arxiv.org/abs/2104.04132'); return false;">Replay in deep learning: current approaches and missing biological elements</a>. In: Neural Computation, 2021</li>
				<li><strong>T.L. Hayes</strong> and C. Kanan. <a href="https://arxiv.org/abs/2103.03987" onclick="getOutboundLink('https://arxiv.org/abs/2103.03987'); return false;">Selective replay enhances learning in online continual analogical reasoning</a>. CVPR Workshop: Continual Learning in Computer Vision, 2021</li>
				<li>V. Lomonaco, L. Pellegrini, A. Cossu, A. Carta, G. Graffieti, <strong>T.L. Hayes</strong>, M. De Lange, M. Masana, J. Pomponi, G. van de Ven, M. Mundt, Q. She, K. Cooper, J. Forest, E. Belouadah, S. Calderara, G.I. Parisi, F. Cuzzolin, A. Tolias, S. Scardapane, L. Antiga, S. Amhad, A. Popescu, C. Kanan, J. van de Weijer, T. Tuytelaars, D. Bacciu, and D. Maltoni <a href="https://arxiv.org/abs/2104.00405" onclick="getOutboundLink('https://arxiv.org/abs/2104.00405'); return false;">Avalanche: an end-to-end library for continual learning</a>. CVPR Workshop: Continual Learning in Computer Vision, 2021</li>
				<li>R. Roady, <strong>T.L. Hayes</strong>, R. Kemker, A. Gonzales, and C. Kanan. <a href="https://doi.org/10.1371/journal.pone.0238302" onclick="getOutboundLink('https://doi.org/10.1371/journal.pone.0238302'); return false;">Are open set classification methods effective on large-scale datasets?</a>. In: PLoS ONE, 2020</li>
				<li>M. Acharya, <strong>T.L. Hayes</strong>, and C. Kanan. <a href="https://arxiv.org/abs/2008.06439" onclick="getOutboundLink('https://arxiv.org/abs/2008.06439'); return false;">RODEO: replay for online object detection</a>. In: British Machine Vision Conference (BMVC), 2020</li>
				<li>R. Roady, <strong>T.L. Hayes</strong>, and C. Kanan. <a href="https://arxiv.org/abs/2009.04659" onclick="getOutboundLink('https://arxiv.org/abs/2009.04659'); return false;">Improved robustness to open set inputs via tempered mixup</a>. ECCV Workshop: Adversarial Robustness in the Real World, 2020</li>
				<li><strong>T.L. Hayes*</strong>, K. Kafle*, R. Shrestha*, M. Acharya, and C. Kanan. <a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;">REMIND your neural network to prevent catastrophic forgetting</a>. In: Proc. European Conference on Computer Vision (ECCV), 2020</li>
				<li><strong>T.L. Hayes</strong> and C. Kanan. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html'); return false;">Lifelong machine learning with deep streaming linear discriminant analysis</a>. CVPR Workshop: Continual Learning in Computer Vision, 2020</li>
				<li>R. Roady*, <strong>T.L. Hayes*</strong>, H. Vaidya, and C. Kanan. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html'); return false;">Stream-51: Streaming Classification and Novelty Detection from Videos</a>. CVPR Workshop: Continual Learning in Computer Vision, 2020</li>
				<li><strong>T.L. Hayes</strong>, N.D. Cahill, and C. Kanan. <a href="https://ieeexplore.ieee.org/document/8793982" onclick="getOutboundLink('https://ieeexplore.ieee.org/document/8793982'); return false;">Memory efficient experience replay for streaming learning</a>. In: Proc. IEEE International Conference on Robotics and Automation (ICRA), 2019</li>
				<li><strong>T.L. Hayes</strong>, R. Kemker, N.D. Cahill, and C. Kanan. <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html'); return false;">New metrics and experimental paradigms for continual learning</a>. CVPR Workshop: Real-World Challenges and New Benchmarks for Deep Learning in Robotic Vision, 2018</li>
				<li>N.D. Cahill, <strong>T.L. Hayes</strong>, R.T. Meinhold, and J.F. Hamilton. <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html'); return false;">Compassionately conservative balanced cuts for image segmentation</a>. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018</li>
				<li>R. Kemker, M. McClure, A. Abitino, <strong>T.L. Hayes</strong>, and C. Kanan. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410" onclick="getOutboundLink('https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410'); return false;">Measuring catastrophic forgetting in neural networks</a>. In: AAAI, 2018</li>
			</ul></h5>

			<h4>Pre-Prints</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes</strong>, M. Nickel, C. Kanan, L. Denoyer, and A. Szlam. <a href="http://arxiv.org/abs/2203.06215" onclick="getOutboundLink('http://arxiv.org/abs/2203.06215'); return false;">Can I see an example? Active learning the long tail of attributes and relations</a>. 2022</li>
				<li>Z. Qian, <strong>T.L. Hayes</strong>, K. Kafle, and C. Kanan. <a href="https://arxiv.org/abs/2004.13587" onclick="getOutboundLink('https://arxiv.org/abs/2004.13587'); return false;">Do we need fully connected output layers in convolutional networks?</a>. 2020</li>
			</ul></h5>

			<h4>Conference Papers</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes</strong>, R.T. Meinhold, J.F. Hamilton, and N.D. Cahill. <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10198/101980O/Piecewise-flat-embeddings-for-hyperspectral-image-analysis/10.1117/12.2262302.full" onclick="getOutboundLink('https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10198/101980O/Piecewise-flat-embeddings-for-hyperspectral-image-analysis/10.1117/12.2262302.full'); return false;">Piecewise flat embeddings for hyperspectral image analysis</a>. In: Proc. SPIE DCS Defense and Security: Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIII, 2017</li>
				<li>R.T. Meinhold, <strong>T.L. Hayes</strong>, and N.D. Cahill. <a href="https://arxiv.org/abs/1612.06496" onclick="getOutboundLink('https://arxiv.org/abs/1612.06496'); return false;">Efficiently computing piecewise flat embeddings for data clustering and image segmentation</a>. In: Proc. IEEE MIT Undergraduate Research and Technology Conference, 2016</li>
			</ul></h5>

			<h4>Theses</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes</strong>. <a href="https://scholarworks.rit.edu/theses/11115/" onclick="getOutboundLink('https://scholarworks.rit.edu/theses/11115/'); return false;">Towards efficient lifelong machine learning in deep neural networks</a>. Doctoral Dissertation, Rochester Institute of Technology, 2022</li>
				<li><strong>T.L. Hayes</strong>. <a href="http://scholarworks.rit.edu/theses/9409/" onclick="getOutboundLink('http://scholarworks.rit.edu/theses/9409/'); return false;">Compassionately conservative normalized cuts for image segmentation</a>. M.S. Thesis, Rochester Institute of Technology, 2017</li>
			</ul></h5>
		</div>
	</div>
</div>

<footer class="container-fluid text-center">
  <a href="#myPage" title="To Top">
    <span class="glyphicon glyphicon-chevron-up"></span>
  </a>
</footer>

<script>
$(document).ready(function(){
  // Add smooth scrolling to all links in navbar + footer link
  $(".navbar a, footer a[href='#myPage']").on('click', function(event) {
    // Make sure this.hash has a value before overriding default behavior
    if (this.hash !== "") {
      // Prevent default anchor click behavior
      event.preventDefault();

      // Store hash
      var hash = this.hash;

      // Using jQuery's animate() method to add smooth page scroll
      // The optional number (900) specifies the number of milliseconds it takes to scroll to the specified area
      $('html, body').animate({
        scrollTop: $(hash).offset().top
      }, 900, function(){

        // Add hash (#) to URL when done scrolling (default click behavior)
        window.location.hash = hash;
      });
    } // End if
  });

  $(window).scroll(function() {
    $(".slideanim").each(function(){
      var pos = $(this).offset().top;

      var winTop = $(window).scrollTop();
        if (pos < winTop + 600) {
          $(this).addClass("slide");
        }
    });
  });
})
</script>

</body>
</html>
